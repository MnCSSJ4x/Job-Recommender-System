{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\nimport pandas as pd\ntokenizer = AutoTokenizer.from_pretrained(\"MohammedDhiyaEddine/job-skill-sentence-transformer-tsdae\")\nmodel = AutoModel.from_pretrained(\"MohammedDhiyaEddine/job-skill-sentence-transformer-tsdae\").to('cuda')","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:13:35.687754Z","iopub.execute_input":"2023-04-20T14:13:35.688281Z","iopub.status.idle":"2023-04-20T14:13:40.949188Z","shell.execute_reply.started":"2023-04-20T14:13:35.688230Z","shell.execute_reply":"2023-04-20T14:13:40.947657Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# df=pd.read_csv('/kaggle/input/job-recommendation-case-study/jobs.tsv', sep='\\t')\ndf = pd.read_csv('/kaggle/input/job-recommendation-case-study/jobs.tsv',sep = '\\t',usecols=range(3,4)).astype(str)#.head(100)\ndf['Cleaned_Description'] = ''\ndf.head()\ndf\n# del df","metadata":{"execution":{"iopub.status.busy":"2023-04-17T05:50:27.028391Z","iopub.execute_input":"2023-04-17T05:50:27.028937Z","iopub.status.idle":"2023-04-17T05:51:52.523041Z","shell.execute_reply.started":"2023-04-17T05:50:27.028898Z","shell.execute_reply":"2023-04-17T05:51:52.521877Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                               Description Cleaned_Description\n0        <p>Security Clearance Required:&nbsp; Top Secr...                    \n1        <strong>NO Corp. to Corp resumes&nbsp;are bein...                    \n2        <b>    <b> P/T HUMAN RESOURCES ASSISTANT</b> <...                    \n3        CITY BEVERAGES Come to work for the best in th...                    \n4        I make  sure every part of their day is magica...                    \n...                                                    ...                 ...\n1091921  <p><strong><span style=\"text-decoration: under...                    \n1091922  <div style=\"text-align: center\"><span style=\"t...                    \n1091923  <span>\\r<hr>\\r<p align=\"center\"><strong>Assist...                    \n1091924  <p>&nbsp;</p>\\r<p><b><span>Macomb Community Co...                    \n1091925  <p><strong>\\r</strong></p>\\r<p><strong><strong...                    \n\n[1091926 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Description</th>\n      <th>Cleaned_Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;p&gt;Security Clearance Required:&amp;nbsp; Top Secr...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;strong&gt;NO Corp. to Corp resumes&amp;nbsp;are bein...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;b&gt;    &lt;b&gt; P/T HUMAN RESOURCES ASSISTANT&lt;/b&gt; &lt;...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CITY BEVERAGES Come to work for the best in th...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I make  sure every part of their day is magica...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1091921</th>\n      <td>&lt;p&gt;&lt;strong&gt;&lt;span style=\"text-decoration: under...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1091922</th>\n      <td>&lt;div style=\"text-align: center\"&gt;&lt;span style=\"t...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1091923</th>\n      <td>&lt;span&gt;\\r&lt;hr&gt;\\r&lt;p align=\"center\"&gt;&lt;strong&gt;Assist...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1091924</th>\n      <td>&lt;p&gt;&amp;nbsp;&lt;/p&gt;\\r&lt;p&gt;&lt;b&gt;&lt;span&gt;Macomb Community Co...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1091925</th>\n      <td>&lt;p&gt;&lt;strong&gt;\\r&lt;/strong&gt;&lt;/p&gt;\\r&lt;p&gt;&lt;strong&gt;&lt;strong...</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n<p>1091926 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import re\nfrom bs4 import BeautifulSoup\ndef cleanResume(resumeText):\n    soup=BeautifulSoup(resumeText,\"html.parser\")\n    resumeText=soup.get_text()\n    resumeText = re.sub('http\\S+\\s*', ' ', resumeText)  # remove URLs\n    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n    resumeText = re.sub('#\\S+', '', resumeText)  # remove hashtags\n    resumeText = re.sub('@\\S+', '  ', resumeText)  # remove mentions\n    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n    resumeText = re.sub(r'[^\\x00-\\x7f]',r' ', resumeText) \n    resumeText = re.sub('\\s+', ' ', resumeText)  # remove extra whitespace\n    return resumeText\n    \ndf['Cleaned_Description'] = df.Description.apply(lambda x: cleanResume(x))\ndf\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Source'] = 'KaggleManda'\ndf.drop([\"Description\"],inplace=True,axis=1)\n# df = df.rename(columns={'old_name': 'new_name'})\ndf# assign values to new column","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv(\"KaggleManda.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del df\ndf=pd.read_csv(\"/kaggle/input/scrapeddata/cleaned_jobs.csv\").drop([\"Unnamed: 0\",\"Location\",\"Company\",\"Position\"],axis=1)\n# df=pd.read_csv(\"/kaggle/input/kagglemanda/KaggleManda_embeddings.csv\").drop([\"Source\"],axis=1).dropna()\n# df = df.rename(columns={'Cleaned_Resume': 'Cleaned_Description'})\ndf","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:53:55.071527Z","iopub.execute_input":"2023-04-20T14:53:55.072598Z","iopub.status.idle":"2023-04-20T14:53:55.136359Z","shell.execute_reply.started":"2023-04-20T14:53:55.072546Z","shell.execute_reply":"2023-04-20T14:53:55.135222Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                                 clean\n0    About Accenture Accenture is a global professi...\n1    Skill required Clinical SDTM Programming Desig...\n2    About Accenture Accenture is a global professi...\n3    Detailed JD Develop procedure and functions us...\n4    About Accenture Accenture is a global professi...\n..                                                 ...\n823  Developer is responsible for creating Azure Da...\n824  This job is sourced from a job board Learn mor...\n825  Job Description The ideal candidate for this p...\n826  Position Azure Data Engineer Mandatory Skills ...\n827  ABOUT US Formed with the vision of helping bra...\n\n[828 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>About Accenture Accenture is a global professi...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Skill required Clinical SDTM Programming Desig...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>About Accenture Accenture is a global professi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Detailed JD Develop procedure and functions us...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>About Accenture Accenture is a global professi...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>823</th>\n      <td>Developer is responsible for creating Azure Da...</td>\n    </tr>\n    <tr>\n      <th>824</th>\n      <td>This job is sourced from a job board Learn mor...</td>\n    </tr>\n    <tr>\n      <th>825</th>\n      <td>Job Description The ideal candidate for this p...</td>\n    </tr>\n    <tr>\n      <th>826</th>\n      <td>Position Azure Data Engineer Mandatory Skills ...</td>\n    </tr>\n    <tr>\n      <th>827</th>\n      <td>ABOUT US Formed with the vision of helping bra...</td>\n    </tr>\n  </tbody>\n</table>\n<p>828 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# df2.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merged_df =merged_df = pd.concat([df1, df2])\n# merged_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merged_df.to_csv(\"Merged.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/merged/Merged.csv\").drop([\"Source\"],axis=1).dropna()\ndf","metadata":{"execution":{"iopub.status.busy":"2023-04-17T04:16:18.596315Z","iopub.execute_input":"2023-04-17T04:16:18.596728Z","iopub.status.idle":"2023-04-17T04:16:23.029210Z","shell.execute_reply.started":"2023-04-17T04:16:18.596690Z","shell.execute_reply":"2023-04-17T04:16:23.027334Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/2879270791.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/merged/Merged.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Source\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."],"ename":"ParserError","evalue":"Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.","output_type":"error"}]},{"cell_type":"code","source":"df.iloc[22603]","metadata":{"execution":{"iopub.status.busy":"2023-04-16T17:09:12.412121Z","iopub.execute_input":"2023-04-16T17:09:12.412583Z","iopub.status.idle":"2023-04-16T17:09:12.420699Z","shell.execute_reply.started":"2023-04-16T17:09:12.412537Z","shell.execute_reply":"2023-04-16T17:09:12.419729Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"Cleaned_Description     Responsible for reconciling an assigned Natio...\nName: 22604, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"list_of_lists =[item for sublist in df.values.tolist() for item in sublist]\nlen(list_of_lists)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:54:01.999419Z","iopub.execute_input":"2023-04-20T14:54:01.999884Z","iopub.status.idle":"2023-04-20T14:54:02.010168Z","shell.execute_reply.started":"2023-04-20T14:54:01.999842Z","shell.execute_reply":"2023-04-20T14:54:02.009033Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"828"},"metadata":{}}]},{"cell_type":"code","source":"# print(len(temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\nimport numpy as np\nn=int(df.shape[0])\nmax_len=512\nembeddings=torch.Tensor([]).to('cuda')#(np.zeros((n,df.shape[1],max_len,768)))\nfor i in range(n):\n    sentences =list_of_lists[i]\n    encoded_sentences = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\",max_length=max_len)\n#     encoded_sentences = tokenizer(sentences, return_tensors=\"pt\")\n    encoded_sentences.to('cuda')\n#     print(len(encoded_sentences))\n#     print(i)\n#     print(\"encoded_sentences size=\",len(encoded_sentences[0]),len(encoded_sentences['input_ids']),len(encoded_sentences['input_ids']))\n    with torch.no_grad():\n#         temp=[]\n#         print(model(encoded_sentences['input_ids'], attention_mask=encoded_sentences['attention_mask'])[0])\n        temp=model(**encoded_sentences)\n#     print(model(**encoded_sentences)[0])\n    sentence_embeddings = mean_pooling(temp, encoded_sentences['attention_mask'])\n#         print(temp)\n#     embeddings[i]=temp\n#     embeddings.append(temp)\n    embeddings=torch.cat((embeddings,sentence_embeddings),0)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:54:51.540923Z","iopub.execute_input":"2023-04-20T14:54:51.541526Z","iopub.status.idle":"2023-04-20T14:55:02.199188Z","shell.execute_reply.started":"2023-04-20T14:54:51.541488Z","shell.execute_reply":"2023-04-20T14:55:02.198191Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print(len(embeddings))\nprint(len(embeddings[0]))\n# print(len(embeddings[0][0]))\n# print(len(embeddings[0][0][0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:55:02.201085Z","iopub.execute_input":"2023-04-20T14:55:02.201503Z","iopub.status.idle":"2023-04-20T14:55:02.208862Z","shell.execute_reply.started":"2023-04-20T14:55:02.201443Z","shell.execute_reply":"2023-04-20T14:55:02.207728Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"828\n768\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(embeddings[0][0][0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n# Open a file for writing\nwith open('jobEmbeddingArya.pickle', 'wb') as f:\n    # Use pickle to dump the variable to the file\n    pickle.dump(embeddings, f)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:55:44.977761Z","iopub.execute_input":"2023-04-20T14:55:44.978342Z","iopub.status.idle":"2023-04-20T14:55:44.988918Z","shell.execute_reply.started":"2023-04-20T14:55:44.978288Z","shell.execute_reply":"2023-04-20T14:55:44.987899Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import pickle\n\n# Open the pickle file for reading\nwith open('jobEmbeddingArya.pickle', 'rb') as f:\n#     Use pickle to load the variable from the file\n    embeddings = pickle.load(f)\n\n# Print the loaded variable\nprint(embeddings)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:55:51.157164Z","iopub.execute_input":"2023-04-20T14:55:51.157535Z","iopub.status.idle":"2023-04-20T14:55:51.171154Z","shell.execute_reply.started":"2023-04-20T14:55:51.157501Z","shell.execute_reply":"2023-04-20T14:55:51.169709Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"tensor([[-0.1817,  0.2965, -0.0935,  ..., -0.0338,  0.3113, -0.7726],\n        [-0.0982,  0.2325,  0.0284,  ...,  0.3190,  0.4740, -0.6322],\n        [-0.3339,  0.2290,  0.0205,  ...,  0.2724,  0.2455, -0.5730],\n        ...,\n        [-0.2738,  0.6630, -0.5232,  ...,  0.0710, -0.3149, -0.2214],\n        [-0.4214, -0.1128, -0.6639,  ..., -0.6259,  0.1240, -0.5204],\n        [-0.1358,  0.1060, -0.7171,  ..., -0.4843,  0.2236, -0.3398]],\n       device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"# list_of_lists[22603]","metadata":{"execution":{"iopub.status.busy":"2023-04-16T17:07:39.084063Z","iopub.execute_input":"2023-04-16T17:07:39.084583Z","iopub.status.idle":"2023-04-16T17:07:39.094534Z","shell.execute_reply.started":"2023-04-16T17:07:39.084542Z","shell.execute_reply":"2023-04-16T17:07:39.092711Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"nan"},"metadata":{}}]},{"cell_type":"code","source":"# # for my_string in list_of_lists[0]:\n# #         words = my_string.split()\n# # len(words)\n# list1=[]\n# list1.append(list_of_lists[22603].split())\n# # list1.append(list_of_lists[1].split())\n# list1","metadata":{"execution":{"iopub.status.busy":"2023-04-16T17:07:27.526650Z","iopub.execute_input":"2023-04-16T17:07:27.527123Z","iopub.status.idle":"2023-04-16T17:07:27.556771Z","shell.execute_reply.started":"2023-04-16T17:07:27.527084Z","shell.execute_reply":"2023-04-16T17:07:27.555363Z"},"trusted":true},"execution_count":36,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3644684321.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# len(words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlist1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlist1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m22603\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# list1.append(list_of_lists[1].split())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlist1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"],"ename":"AttributeError","evalue":"'float' object has no attribute 'split'","output_type":"error"}]},{"cell_type":"code","source":"list1=[]\nfor i in range(len(list_of_lists)):\n#         words = my_string.split()\n#         print(my_string)\n#     if(i>22600 and i<22700):\n#         print(i)\n    list1.append(list_of_lists[i].split())\nlen(list1)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:14:13.980986Z","iopub.execute_input":"2023-04-20T14:14:13.982172Z","iopub.status.idle":"2023-04-20T14:14:14.021924Z","shell.execute_reply.started":"2023-04-20T14:14:13.982085Z","shell.execute_reply":"2023-04-20T14:14:14.020010Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"828"},"metadata":{}}]},{"cell_type":"code","source":"corpus=[item for sublist in list1 for item in sublist]\nlen(corpus)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:14:16.527338Z","iopub.execute_input":"2023-04-20T14:14:16.527883Z","iopub.status.idle":"2023-04-20T14:14:16.549839Z","shell.execute_reply.started":"2023-04-20T14:14:16.527838Z","shell.execute_reply":"2023-04-20T14:14:16.547917Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"216857"},"metadata":{}}]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n  \nstop_words = set(stopwords.words('english'))\nfiltered_sentence = [w for w in list1 if not w.lower() in stop_words]\nlen(filtered_sentence)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:14:19.741028Z","iopub.status.idle":"2023-04-20T14:14:19.742431Z","shell.execute_reply.started":"2023-04-20T14:14:19.742115Z","shell.execute_reply":"2023-04-20T14:14:19.742157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import Word2Vec\n# my_string = \"This is a string with spaces\"\n# list1=[item for sublist in list1 for item in sublist]\n# Train a Word2Vec model on the sentences\nmodel = Word2Vec(vector_size=300,window=5,min_count=5,workers=-1)\nmodel.build_vocab(corpus)\nmodel.train(corpus,total_examples=model.corpus_count,epochs=5)\nmodel.save(\"word2vecArya.model\")\nmodel = Word2Vec.load(\"word2vecArya.model\")\n# Get the w2v representation of a sentence\n# sentence = ['this', 'is', 'sentence', 'one']\n# w2v_representation = model.wv[list_of_lists[0][0]]\n# for word in list1:\n#   vector = model.wv[word]\n\n# print(vector)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:18:47.848845Z","iopub.execute_input":"2023-04-20T14:18:47.849815Z","iopub.status.idle":"2023-04-20T14:18:49.302128Z","shell.execute_reply.started":"2023-04-20T14:18:47.849760Z","shell.execute_reply":"2023-04-20T14:18:49.271577Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf=TfidfVectorizer(analyzer='word',ngram_range=(1,2),min_df=25,stop_words=\"english\")\ntfidf.fit(df['clean'])\ntfidf_list=dict(zip(tfidf.get_feature_names_out(),list(tfidf.idf_)))\ntfidf_feature=tfidf.get_feature_names_out()","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:18:49.433018Z","iopub.execute_input":"2023-04-20T14:18:49.433497Z","iopub.status.idle":"2023-04-20T14:18:49.912084Z","shell.execute_reply.started":"2023-04-20T14:18:49.433455Z","shell.execute_reply":"2023-04-20T14:18:49.910476Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\ntfidf_vectors=[]\nline=0\nfor desc in tqdm(corpus):\n    sent_vec=np.zeros(300)\n    weight_sum=0\n    for word in desc:\n        if word in model.wv.key_to_index and word in tfidf_feature:\n            vec=model.wv[word]\n            tf_idf=tfidf_list[word]*(desc.count(word)/len(desc))\n            sent_vec+=(vec*tf_idf)\n            weight_sum+=tf_idf\n        if weight_sum!=0:\n            sent_vec/=weight_sum\n        tfidf_vectors.append(sent_vec)\n        line+=1","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:18:50.851450Z","iopub.execute_input":"2023-04-20T14:18:50.852897Z","iopub.status.idle":"2023-04-20T14:19:24.340404Z","shell.execute_reply.started":"2023-04-20T14:18:50.852821Z","shell.execute_reply":"2023-04-20T14:19:24.338779Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"100%|██████████| 216857/216857 [00:33<00:00, 6500.59it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"len(tfidf_vectors)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:18:35.124915Z","iopub.execute_input":"2023-04-20T14:18:35.125348Z","iopub.status.idle":"2023-04-20T14:18:35.134942Z","shell.execute_reply.started":"2023-04-20T14:18:35.125308Z","shell.execute_reply":"2023-04-20T14:18:35.133065Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"1222282"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n# cosine_similarity=cosine_similarity(tfidf_vectors,tfidf_vectors)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:03:34.200332Z","iopub.execute_input":"2023-04-20T15:03:34.201226Z","iopub.status.idle":"2023-04-20T15:03:34.809837Z","shell.execute_reply.started":"2023-04-20T15:03:34.201175Z","shell.execute_reply":"2023-04-20T15:03:34.808831Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Dealing with user_embedding","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\nimport pandas as pd\ntokenizer = AutoTokenizer.from_pretrained(\"MohammedDhiyaEddine/job-skill-sentence-transformer-tsdae\")\nmodel = AutoModel.from_pretrained(\"MohammedDhiyaEddine/job-skill-sentence-transformer-tsdae\").to('cuda')","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:41:15.673819Z","iopub.execute_input":"2023-04-20T14:41:15.674190Z","iopub.status.idle":"2023-04-20T14:41:31.184540Z","shell.execute_reply.started":"2023-04-20T14:41:15.674159Z","shell.execute_reply":"2023-04-20T14:41:31.183401Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d88244c6d2224836bb857b6801cc37d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67b7b1f2471944afb5346006c9b998ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd7e77a6d8a64be5af78419e20847c09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"074571f5262e4209991db2df18de6b35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a5083b8dd7449dfaecee685b8f74a2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5001f5345c846858f1dc3f231ed2b3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a69d56b581ad43d0ad664e36f08a77e9"}},"metadata":{}}]},{"cell_type":"code","source":"user_df=pd.read_csv(\"/kaggle/input/userresume/UserResume.csv\").dropna().drop([\"Resume\"],axis=1)\n# user_df[\"Combined\"] = df[\"Category\"].astype(str) + df[\"Cleaned_Resume\"]\nuser_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:41:31.187627Z","iopub.execute_input":"2023-04-20T14:41:31.188443Z","iopub.status.idle":"2023-04-20T14:41:31.362988Z","shell.execute_reply.started":"2023-04-20T14:41:31.188383Z","shell.execute_reply":"2023-04-20T14:41:31.361802Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"       Category                                     Cleaned_Resume\n0  Data Science  Skills Programming Languages Python pandas num...\n1  Data Science  Education Details May 2013 to May 2017 B E UIT...\n2  Data Science  Areas of Interest Deep Learning Control System...\n3  Data Science  Skills R Python SAP HANA Tableau SAP HANA SQL ...\n4  Data Science  Education Details MCA YMCAUST Faridabad Haryan...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Category</th>\n      <th>Cleaned_Resume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Data Science</td>\n      <td>Skills Programming Languages Python pandas num...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Data Science</td>\n      <td>Education Details May 2013 to May 2017 B E UIT...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Data Science</td>\n      <td>Areas of Interest Deep Learning Control System...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Data Science</td>\n      <td>Skills R Python SAP HANA Tableau SAP HANA SQL ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Data Science</td>\n      <td>Education Details MCA YMCAUST Faridabad Haryan...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"user_df[\"Combined\"] = user_df[\"Category\"].astype(str) + user_df[\"Cleaned_Resume\"]\nuser_df=user_df.drop([\"Category\",\"Cleaned_Resume\"],axis=1)\nuser_df","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:41:31.364561Z","iopub.execute_input":"2023-04-20T14:41:31.364937Z","iopub.status.idle":"2023-04-20T14:41:31.384029Z","shell.execute_reply.started":"2023-04-20T14:41:31.364898Z","shell.execute_reply":"2023-04-20T14:41:31.383121Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                              Combined\n0    Data ScienceSkills Programming Languages Pytho...\n1    Data ScienceEducation Details May 2013 to May ...\n2    Data ScienceAreas of Interest Deep Learning Co...\n3    Data ScienceSkills R Python SAP HANA Tableau S...\n4    Data ScienceEducation Details MCA YMCAUST Fari...\n..                                                 ...\n957  TestingComputer Skills Proficient in MS office...\n958  Testing Willingness to a ept the challenges Po...\n959  TestingPERSONAL SKILLS Quick learner Eagerness...\n960  TestingCOMPUTER SKILLS SOFTWARE KNOWLEDGE MS P...\n961  TestingSkill Set OS Windows XP 7 8 8 1 10 Data...\n\n[962 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Combined</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Data ScienceSkills Programming Languages Pytho...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Data ScienceEducation Details May 2013 to May ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Data ScienceAreas of Interest Deep Learning Co...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Data ScienceSkills R Python SAP HANA Tableau S...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Data ScienceEducation Details MCA YMCAUST Fari...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>957</th>\n      <td>TestingComputer Skills Proficient in MS office...</td>\n    </tr>\n    <tr>\n      <th>958</th>\n      <td>Testing Willingness to a ept the challenges Po...</td>\n    </tr>\n    <tr>\n      <th>959</th>\n      <td>TestingPERSONAL SKILLS Quick learner Eagerness...</td>\n    </tr>\n    <tr>\n      <th>960</th>\n      <td>TestingCOMPUTER SKILLS SOFTWARE KNOWLEDGE MS P...</td>\n    </tr>\n    <tr>\n      <th>961</th>\n      <td>TestingSkill Set OS Windows XP 7 8 8 1 10 Data...</td>\n    </tr>\n  </tbody>\n</table>\n<p>962 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"list_of_lists =[item for sublist in user_df.values.tolist() for item in sublist]\nlen(list_of_lists)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:41:31.387873Z","iopub.execute_input":"2023-04-20T14:41:31.388151Z","iopub.status.idle":"2023-04-20T14:41:31.396548Z","shell.execute_reply.started":"2023-04-20T14:41:31.388125Z","shell.execute_reply":"2023-04-20T14:41:31.395235Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"962"},"metadata":{}}]},{"cell_type":"code","source":"def mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\nimport numpy as np\nn=int(user_df.shape[0])\nmax_len=512\nembeddings=torch.Tensor([]).to('cuda')#(np.zeros((n,df.shape[1],max_len,768)))\nfor i in range(n):\n    sentences =list_of_lists[i]\n    encoded_sentences = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\",max_length=max_len)\n    encoded_sentences.to('cuda')\n    with torch.no_grad():\n        temp=model(**encoded_sentences)\n    sentence_embeddings = mean_pooling(temp, encoded_sentences['attention_mask'])\n    embeddings=torch.cat((embeddings,sentence_embeddings),0)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:41:31.398573Z","iopub.execute_input":"2023-04-20T14:41:31.399429Z","iopub.status.idle":"2023-04-20T14:41:48.678049Z","shell.execute_reply.started":"2023-04-20T14:41:31.399392Z","shell.execute_reply":"2023-04-20T14:41:48.676936Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import pickle\n# Open a file for writing\nwith open('userEmbeddingArya.pickle', 'wb') as f:\n    # Use pickle to dump the variable to the file\n    pickle.dump(embeddings, f)","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:42:42.194260Z","iopub.execute_input":"2023-04-20T14:42:42.195198Z","iopub.status.idle":"2023-04-20T14:42:42.206422Z","shell.execute_reply.started":"2023-04-20T14:42:42.195160Z","shell.execute_reply":"2023-04-20T14:42:42.205252Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import pickle\n\n# Open the pickle file for reading\nwith open('userEmbeddingArya.pickle', 'rb') as f:\n    # Use pickle to load the variable from the file\n    embeddings = pickle.load(f)\n\n# Print the loaded variable\nprint(len(embeddings),len(embeddings[0]))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:42:43.335866Z","iopub.execute_input":"2023-04-20T14:42:43.336234Z","iopub.status.idle":"2023-04-20T14:42:43.347627Z","shell.execute_reply.started":"2023-04-20T14:42:43.336200Z","shell.execute_reply":"2023-04-20T14:42:43.346390Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"962 768\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Stacking user and job embeddings","metadata":{}},{"cell_type":"code","source":"import pickle\n\n# Open the pickle file for reading\nwith open('/kaggle/input/embeddings/userEmbeddingArya.pickle', 'rb') as f:\n    # Use pickle to load the variable from the file\n    user_embeddings = pickle.load(f)\n\nwith open('/kaggle/input/embeddings/jobEmbeddingArya.pickle', 'rb') as f:\n    # Use pickle to load the variable from the file\n    job_embeddings = pickle.load(f)\n    \n# Print the loaded variable\nprint(len(user_embeddings),len(user_embeddings[0]))\nprint(len(job_embeddings),len(job_embeddings[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-20T14:58:04.184352Z","iopub.execute_input":"2023-04-20T14:58:04.185073Z","iopub.status.idle":"2023-04-20T14:58:04.282759Z","shell.execute_reply.started":"2023-04-20T14:58:04.185036Z","shell.execute_reply":"2023-04-20T14:58:04.281647Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"962 768\n828 768\n","output_type":"stream"}]},{"cell_type":"code","source":"import itertools\nimport pandas as pd\n\n# Create two example tensors\ntensor1 = user_embeddings\ntensor2 = job_embeddings\n\n# Generate all permutations of the two tensors\npermutations = itertools.product(tensor1, tensor2)\n\n# Convert tuples to rows in a DataFrame\nEmbedding_df = pd.DataFrame(permutations, columns=['tensor1', 'tensor2'])\n\n# Display the resulting DataFrame\nprint(Embedding_df.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:01:48.268737Z","iopub.execute_input":"2023-04-20T15:01:48.269630Z","iopub.status.idle":"2023-04-20T15:01:49.789884Z","shell.execute_reply.started":"2023-04-20T15:01:48.269584Z","shell.execute_reply":"2023-04-20T15:01:49.788573Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"(796536, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn.functional as F\nEmbedding_df['similarity'] = F.cosine_similarity(Embedding_df['tensor1'], Embedding_df['tensor2'])","metadata":{"execution":{"iopub.status.busy":"2023-04-20T15:05:35.013396Z","iopub.execute_input":"2023-04-20T15:05:35.014452Z","iopub.status.idle":"2023-04-20T15:05:35.046740Z","shell.execute_reply.started":"2023-04-20T15:05:35.014404Z","shell.execute_reply":"2023-04-20T15:05:35.045350Z"},"trusted":true},"execution_count":31,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/2501941707.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mEmbedding_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'similarity'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tensor1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbedding_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tensor2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: cosine_similarity(): argument 'x1' (position 1) must be Tensor, not Series"],"ename":"TypeError","evalue":"cosine_similarity(): argument 'x1' (position 1) must be Tensor, not Series","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}